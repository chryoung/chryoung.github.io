---
title: Recurrent Neural Network (RNN)
mathjax: true
tags: deep-learning
---

## Illustration

<p align="center">
  <img alt="RNN" src="/assets/deep-learning/rnn-render.svg" />
  <br/>
  <i>Fig 1. A many to many RNN</i>
</p>

## Notation

$$
x^{<1>}, x^{<2>}, x^{<3>}, \ldots , x^{<t>}, \ldots, x^{<q>}
$$

means $x$ value on the timestamp $t$.

$$
y^{<1>}, y^{<2>}, y^{<3>}, \ldots, y^{<t>}, \ldots, y^{<q>}
$$

means $y$ value on the timestamp $t$.

$x^{(i)}$ means the ith training example.

$T_x^{(i)} = q$ is the length of sequence for the ith training example $x$.

$T_y$ is the length of sequence  of the outputs.

## Equation

###  Forward Propagation

$$
\begin{align}
a^{<0>} & = \overrightarrow{0} \\
a^{<1>} & = g( W_{aa} a^{<0>} + W_{ax} X^{<1>} + b_a)\\
\hat{y}^{<1>} & = h(W_{ya}a^{<1>} + b_y) \\
& \vdots \\
a^{<t>} & = g(W_{aa} a^{<t-1>} + W_{ax} X^{<t>} + b_a) \\
\hat{y}^{<t>} & = h(W_{ya} a^{<t>} + b_y)
\end{align}
$$

{% raw %}

where
- $g(z)$ is $\tanh$ or ReLu.
- $h(z)$ is sigmoid or softmax.
- $X^{\<t\>}$ is the input on the timestamp $t$.
- $a^{\<t\>}$ is the value after activation on the timestamp $t$.
- $\hat{y}^{\<t\>}$ is the output on the timestamp $t$.
- $W_{aa}$ is the weight with respect to $a^{\<t\>}$.
- $W_{ax}$ is the weight with respect to $X^{\<t\>}$.
- $b_a$ is the bias of $a^{\<t\>}$.
- $b_y$ is the bias of $\hat{y}^{\<t\>}$.

{% endraw %}

### Backpropagation

Cross entropy loss function for RNN:

$$
\begin{align}
L^{<t>}(\hat{y}^{<t>}, y^{<t>}) & = -y^{<t>} \log \hat{y}^{<t>} - (1 - y^{<t>}) \log (1 - \hat{y}^{<t>}) \\
L(\hat{y}, y) & = \sum^{Ty}_{t=1} L^{<t>} (\hat{y}^{<t>}, y^{<t>}) \\
\end{align}
$$

Consider a simple case where $T = 2$.

$$
\begin{align}
L^{<1>}(\hat{y}^{<1>}, y^{<1>}) & = & -y^{<1>} \log \hat{y}^{<1>} - (1 - y^{<1>}) \log (1 - \hat{y}^{<1>}) \\
L^{<2>}(\hat{y}^{<2>}, y^{<2>}) & = & -y^{<2>} \log \hat{y}^{<2>} - (1 - y^{<2>}) \log (1 - \hat{y}^{<2>}) \\
L(\hat{y}, y) & = & L^{<1>}(\hat{y}^{<1>}, y^{<1>})
+
L^{<2>}(\hat{y}^{<1>}, y^{<1>}) \\
& = & \left( -y^{<1>} \log \hat{y}^{<1>} - (1 - y^{<1>}) \log (1 - \hat{y}^{<1>}) \right) \\
& & + \left( -y^{<2>} \log \hat{y}^{<2>} - (1 - y^{<2>}) \log (1 - \hat{y}^{<2>}) \right)
\end{align}
$$

To get the partial derivate of the loss function, use the chain rule.

$$
\begin{align}
\frac{\partial L^{<2>}}{\partial W_{aa}} & = & \frac{\partial L^{<2>}}{\partial \hat{y}^{<2>}} \frac{\partial \hat{y}^{<2>}}{\partial W_{aa}} \\
& = & \frac{\partial L^{<2>}}{\partial \hat{y}^{<2>}} \frac{\partial \hat{y}^{<2>}}{\partial a^{<2>}} \frac{\partial a^{<2>}}{\partial W_{aa}} \\
& = & \frac{\partial L^{<2>}}{\partial \hat{y}^{<2>}} \frac{\partial \hat{y}^{<2>}}{\partial a^{<2>}} \frac{\partial a^{<2>}}{\partial a^{<1>}} \frac{\partial a^{<1>}}{\partial W_{aa}}
\end{align}
$$

As we can see, $\partial L^{\<2\>} / \partial W_{aa}$ requires the derivate with respect to $a^{\<1\>}$. This is called backpropagation through time.

Usually the deep learning framework will do the calculation. We don't need to worry about it.

## Architectures of RNN

There are many architectures of RNN.

1. **Many to Many** ($T_x = T_y$): Words to names.
2. **Many to Many** ($T_y \ne T_y$)
3. **Many to One**: Sentence to rate.
4. **One to One**: Same as standard neural network.
5. **One to Many**: Music generation.

## Why don't just use standard network?

- Inputs and outputs can be different lengths.
- Standard network doesn't share features learned across different position of text.
